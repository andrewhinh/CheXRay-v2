{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <h1 align=\"center\">CheXRay: Automatically Diagnosing Chest X-Rays using Generated Radiologist Reports and Patient Information </h1>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "google_drive_url = \"https://drive.google.com/file/d/1f3GKUpjPKra4NZk5Pjj9OmNcvNkPudzA/view?usp=sharing\"\n",
    "os.environ['GOOGLE_FILE_ID'] = google_drive_url.split('/')[5]\n",
    "os.environ['GDRIVE_URL'] = f'https://docs.google.com/uc?export=download&id={os.environ[\"GOOGLE_FILE_ID\"]}'\n",
    "!wget -q --no-check-certificate $GDRIVE_URL -r -A 'uc*' -e robots=off -nd\n",
    "!mv $(ls -S uc* | head -1) models/sum.0.0.pth\n",
    "\n",
    "!rm -f uc*\n",
    "google_drive_url = \"https://drive.google.com/file/d/13bdY9r8vzpzw_V086ujSwO5WQGz4yo59/view?usp=sharing\"\n",
    "os.environ['GOOGLE_FILE_ID'] = google_drive_url.split('/')[5]\n",
    "os.environ['GDRIVE_URL'] = f'https://docs.google.com/uc?export=download&id={os.environ[\"GOOGLE_FILE_ID\"]}'\n",
    "!wget -q --no-check-certificate $GDRIVE_URL -r -A 'uc*' -e robots=off -nd\n",
    "!mv $(ls -S uc* | head -1) models/repgen.0.0.pth\n",
    "\n",
    "!rm -f uc*\n",
    "google_drive_url = \"https://drive.google.com/file/d/1Pzhd5qdXYWX7zNYBidO-WHKT0CJGJF1H/view?usp=sharing\"\n",
    "os.environ['GOOGLE_FILE_ID'] = google_drive_url.split('/')[5]\n",
    "os.environ['GDRIVE_URL'] = f'https://docs.google.com/uc?export=download&id={os.environ[\"GOOGLE_FILE_ID\"]}'\n",
    "!wget -q --no-check-certificate $GDRIVE_URL -r -A 'uc*' -e robots=off -nd\n",
    "!mv $(ls -S uc* | head -1) models/txtcls.pkl\n",
    "\n",
    "!rm -f uc*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext memory_profiler\n",
    "#from production import run\n",
    "#%mprun -f run run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "#Importing libraries and setup\n",
    "#Modules for helper functions\n",
    "from modules.utils.dicom import * #Because PILDicom from fastai doesn't work\n",
    "from modules.utils.tokenizers import *\n",
    "\n",
    "#Modules for fastai.vis\n",
    "#!pip install -q pydicom pyarrow kornia opencv-python scikit-image nbdev\n",
    "from fastai.basics import *\n",
    "from fastai.callback.all import *\n",
    "from fastai.medical.imaging import *\n",
    "from fastai.vision.widgets import *\n",
    "\n",
    "#Modules for fastai.text\n",
    "from fastai.text.all import *\n",
    "\n",
    "#Modules for fastai.tab\n",
    "from fastai.tabular.all import *\n",
    "\n",
    "#Modules for R2Gen/multimodal\n",
    "from modules.repgen.dataset import RepGenDataset\n",
    "from modules.repgen.dataloader import *\n",
    "import modules.repgen.logits as log\n",
    "from modules.repgen.model import *\n",
    "from modules.repgen.loss import *\n",
    "from modules.repgen.fastai_utils import *\n",
    "from modules.repgen.metrics import bleu4\n",
    "\n",
    "#Modules for sum\n",
    "from modules.sum.dataloader import SumDL  \n",
    "import modules.sum.logits as log1\n",
    "from modules.sum.model import *\n",
    "from modules.sum.loss import *\n",
    "from modules.sum.fastai_utils import *\n",
    "from modules.sum.metrics import *\n",
    "\n",
    "#Other libraries\n",
    "import html\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "import matplotlib.cm as cm\n",
    "import copy as cp\n",
    "import matplotlib.pylab as plt\n",
    "from IPython.display import Image, display, HTML, clear_output\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import language_tool_python\n",
    "tool = language_tool_python.LanguageToolPublicAPI('en-US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making Path object which contains path to data\n",
    "prep = Path('./data/')\n",
    "prod_path = Path('./sample/')\n",
    "classes=[\"Atelectasis\", \"Cardiomegaly\", \"Consolidation\", \"Edema\", \"Enlarged_Cardiomediastinum\", \"Fracture\", \n",
    "         \"Lung_Lesion\", \"Lung_Opacity\", \"No_Finding\", \"Pleural_Effusion\", \"Pleural_Other\", \"Pneumonia\", \n",
    "         \"Pneumothorax\", \"Support_Devices\"]\n",
    "views = ['AP','AP_AXIAL','AP_LLD','AP_RLD','PA','PA_LLD','PA_RLD','LATERAL','LL','LAO','RAO','SWIMMERS','XTABLE_LATERAL','LPO']\n",
    "workers = multiprocessing.cpu_count()\n",
    "defaults.device = torch.device('cpu')\n",
    "device = torch.device(\"cpu\")\n",
    "cpu = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heading = widgets.HTML(value='<style>p{word-wrap: break-word}</style><p>')\n",
    "heading.value += \"This program takes the patient's chest x-ray(s), formatted as .dcm files, as input and<br/>\"\n",
    "heading.value += \"1) generates a radiologist report using the chest x-ray(s),<br/>\"\n",
    "heading.value += \"2) generates tabular data using the time and date,<br/>\"\n",
    "heading.value += \"3) and generates heatmap and intrinsic attention visualizations which represent the diagnosis for the patient using the above-mentioned data.</p>\"\n",
    "heading.value += \"Notes:<br/>\" \n",
    "heading.value += \"- To upload multiple instances of a view, select all of the instances and upload them all at once.<br/>\"\n",
    "heading.value += \"- Although images are saved as files in this website, they are only within your environment.<br/>\"\n",
    "heading.value += \"- Depending on the number of views and images you input, you can expect the program to complete within 10-140 minutes.<br/>\"\n",
    "heading.value += \"If you have any questions or concerns, please contact the author at the the following email: ajhinh@gmail.com.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_direct = widgets.Label()\n",
    "ap_direct.value = \"If (an) AP view instance(s) is/are available, upload it/them here:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_btn_upload = widgets.FileUpload(multiple=True, accept='.dcm') #, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_axial_direct = widgets.Label()\n",
    "ap_axial_direct.value = \"If (an) AP axial view instance(s) is/are available, upload it/them here:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_axial_btn_upload = widgets.FileUpload(multiple=True, accept='.dcm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_lld_direct = widgets.Label()\n",
    "ap_lld_direct.value = \"If (an) AP LLD view instance(s) is/are available, upload it/them here:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_lld_btn_upload = widgets.FileUpload(multiple=True, accept='.dcm') #, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_rld_direct = widgets.Label()\n",
    "ap_rld_direct.value = \"If (an) AP RLD view instance(s) is/are available, upload it/them here:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_rld_btn_upload = widgets.FileUpload(multiple=True, accept='.dcm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_direct = widgets.Label()\n",
    "pa_direct.value = \"If (a) PA view instance(s) is/are available, upload it/them here:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_btn_upload = widgets.FileUpload(multiple=True, accept='.dcm') #, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_lld_direct = widgets.Label()\n",
    "pa_lld_direct.value = \"If (a) PA LLD view instance(s) is/are available, upload it/them here:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_lld_btn_upload = widgets.FileUpload(multiple=True, accept='.dcm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_rld_direct = widgets.Label()\n",
    "pa_rld_direct.value = \"If (a) PA RLD view instance(s) is/are available, upload it/them here:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_rld_btn_upload = widgets.FileUpload(multiple=True, accept='.dcm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_direct = widgets.Label()\n",
    "lat_direct.value = \"If (a) lateral view instance(s) is/are available, upload it/them here:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_btn_upload = widgets.FileUpload(multiple=True, accept='.dcm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_direct = widgets.Label()\n",
    "ll_direct.value = \"If (a) LL view instance(s) is/are available, upload it/them here:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_btn_upload = widgets.FileUpload(multiple=True, accept='.dcm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lao_direct = widgets.Label()\n",
    "lao_direct.value = \"If (a) LAO view instance(s) is/are available, upload it/them here:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lao_btn_upload = widgets.FileUpload(multiple=True, accept='.dcm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rao_direct = widgets.Label()\n",
    "rao_direct.value = \"If (a) RAO view instance(s) is/are available, upload it/them here:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rao_btn_upload = widgets.FileUpload(multiple=True, accept='.dcm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swim_direct = widgets.Label()\n",
    "swim_direct.value = \"If (a) swimmers view instance(s) is/are available, upload it/them here:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swim_btn_upload = widgets.FileUpload(multiple=True, accept='.dcm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtab_lat_direct = widgets.Label()\n",
    "xtab_lat_direct.value = \"If (a) xtable lateral view instance(s) is/are available, upload it/them here:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtab_lat_btn_upload = widgets.FileUpload(multiple=True, accept='.dcm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpo_direct = widgets.Label()\n",
    "lpo_direct.value = \"If (a) LPO view instance(s) is/are available, upload it/them here:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpo_btn_upload = widgets.FileUpload(multiple=True, accept='.dcm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = widgets.HTML(value='<style>p{word-wrap: break-word}</style><p>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_pl = widgets.Output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnose = widgets.Button(description='Diagnose')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_click_classify(change):\n",
    "    input_views = []\n",
    "    input_paths = []\n",
    "    \n",
    "    if ap_btn_upload.data!=[]:\n",
    "        input_views.append(\"AP\")\n",
    "        for path in range(len(ap_btn_upload.data)):\n",
    "            temp_path = prod_path/str('AP_'+str(path)+'.dcm')\n",
    "            input_paths.append(temp_path)\n",
    "            with open(temp_path, 'wb') as f: \n",
    "                f.write(ap_btn_upload.value[list(ap_btn_upload.value.keys())[0]]['content'])\n",
    "    if ap_axial_btn_upload.data!=[]:\n",
    "        input_views.append(\"AP_AXIAL\")\n",
    "        for path in range(len(ap_axial_btn_upload.data)):\n",
    "            temp_path = prod_path/str('AP_AXIAL_'+str(path)+'.dcm')\n",
    "            input_paths.append(temp_path)\n",
    "            with open(temp_path, 'wb') as f: \n",
    "                f.write(ap_axial_btn_upload.value[list(ap_axial_btn_upload.value.keys())[0]]['content'])\n",
    "    if ap_lld_btn_upload.data!=[]:\n",
    "        input_views.append(\"AP_LLD\")\n",
    "        for path in range(len(ap_lld_btn_upload.data)):\n",
    "            temp_path = prod_path/str('AP_LLD_'+str(path)+'.dcm')\n",
    "            input_paths.append(temp_path)\n",
    "            with open(temp_path, 'wb') as f: \n",
    "                f.write(ap_lld_btn_upload.value[list(ap_lld_btn_upload.value.keys())[0]]['content'])\n",
    "    if ap_rld_btn_upload.data!=[]:\n",
    "        input_views.append(\"AP_RLD\")\n",
    "        for path in range(len(ap_rld_btn_upload.data)):\n",
    "            temp_path = prod_path/str('AP_RLD_'+str(path)+'.dcm')\n",
    "            input_paths.append(temp_path)\n",
    "            with open(temp_path, 'wb') as f: \n",
    "                f.write(ap_rld_btn_upload.value[list(ap_rld_btn_upload.value.keys())[0]]['content'])\n",
    "    if pa_btn_upload.data!=[]:\n",
    "        input_views.append(\"PA\")\n",
    "        for path in range(len(pa_btn_upload.data)):\n",
    "            temp_path = prod_path/str('PA_'+str(path)+'.dcm')\n",
    "            input_paths.append(temp_path)\n",
    "            with open(temp_path, 'wb') as f: \n",
    "                f.write(pa_btn_upload.value[list(pa_btn_upload.value.keys())[0]]['content'])\n",
    "    if pa_lld_btn_upload.data!=[]:\n",
    "        input_views.append(\"PA_LLD\")\n",
    "        for path in range(len(pa_lld_btn_upload.data)):\n",
    "            temp_path = prod_path/str('PA_LLD_'+str(path)+'.dcm')\n",
    "            input_paths.append(temp_path)\n",
    "            with open(temp_path, 'wb') as f: \n",
    "                f.write(pa_lld_btn_upload.value[list(pa_lld_btn_upload.value.keys())[0]]['content'])\n",
    "    if pa_rld_btn_upload.data!=[]:\n",
    "        input_views.append(\"PA_RLD\")\n",
    "        for path in range(len(pa_rld_btn_upload.data)):\n",
    "            temp_path = prod_path/str('PA_RLD_'+str(path)+'.dcm')\n",
    "            input_paths.append(temp_path)\n",
    "            with open(temp_path, 'wb') as f: \n",
    "                f.write(pa_rld_btn_upload.value[list(pa_rld_btn_upload.value.keys())[0]]['content'])\n",
    "    if lat_btn_upload.data!=[]:\n",
    "        input_views.append(\"LATERAL\")\n",
    "        for path in range(len(lat_btn_upload.data)):\n",
    "            temp_path = prod_path/str('LATERAL_'+str(path)+'.dcm')\n",
    "            input_paths.append(temp_path)\n",
    "            with open(temp_path, 'wb') as f: \n",
    "                f.write(lat_btn_upload.value[list(lat_btn_upload.value.keys())[0]]['content'])\n",
    "    if ll_btn_upload.data!=[]:\n",
    "        input_views.append(\"LL\")\n",
    "        for path in range(len(ll_btn_upload.data)):\n",
    "            temp_path = prod_path/str('LL_'+str(path)+'.dcm')\n",
    "            input_paths.append(temp_path)\n",
    "            with open(temp_path, 'wb') as f: \n",
    "                f.write(ll_btn_upload.value[list(ll_btn_upload.value.keys())[0]]['content'])\n",
    "    if lao_btn_upload.data!=[]:\n",
    "        input_views.append(\"LAO\")\n",
    "        for path in range(len(lao_btn_upload.data)):\n",
    "            temp_path = prod_path/str('LAO_'+str(path)+'.dcm')\n",
    "            input_paths.append(temp_path)\n",
    "            with open(temp_path, 'wb') as f: \n",
    "                f.write(lao_btn_upload.value[list(lao_btn_upload.value.keys())[0]]['content'])\n",
    "    if rao_btn_upload.data!=[]:\n",
    "        input_views.append(\"RAO\")\n",
    "        for path in range(len(rao_btn_upload.data)):\n",
    "            temp_path = prod_path/str('RAO_'+str(path)+'.dcm')\n",
    "            input_paths.append(temp_path)\n",
    "            with open(temp_path, 'wb') as f: \n",
    "                f.write(rao_btn_upload.value[list(rao_btn_upload.value.keys())[0]]['content'])\n",
    "    if swim_btn_upload.data!=[]:\n",
    "        input_views.append(\"SWIMMERS\")\n",
    "        for path in range(len(swim_btn_upload.data)):\n",
    "            temp_path = prod_path/str('SWIMMERS_'+str(path)+'.dcm')\n",
    "            input_paths.append(temp_path)\n",
    "            with open(temp_path, 'wb') as f: \n",
    "                f.write(swim_btn_upload.value[list(swim_btn_upload.value.keys())[0]]['content'])\n",
    "    if xtab_lat_btn_upload.data!=[]:\n",
    "        input_views.append(\"XTABLE_LATERAL\")\n",
    "        for path in range(len(xtab_lat_btn_upload.data)):\n",
    "            temp_path = prod_path/str('XTABLE_LATERAL_'+str(path)+'.dcm')\n",
    "            input_paths.append(temp_path)\n",
    "            with open(temp_path, 'wb') as f: \n",
    "                f.write(xtab_lat_btn_upload.value[list(xtab_lat_btn_upload.value.keys())[0]]['content'])\n",
    "    if lpo_btn_upload.data!=[]:\n",
    "        input_views.append(\"LPO\")\n",
    "        for path in range(len(lpo_btn_upload.data)):\n",
    "            temp_path = prod_path/str('LPO_'+str(path)+'.dcm')\n",
    "            input_paths.append(temp_path)\n",
    "            with open(temp_path, 'wb') as f: \n",
    "                f.write(lpo_btn_upload.value[list(lpo_btn_upload.value.keys())[0]]['content'])\n",
    "        \n",
    "    nomiss_repgen_trainval_sample_path = prep/'trainval_sample_repgen_nomiss.csv'\n",
    "    trainval_sample = pd.read_csv(nomiss_repgen_trainval_sample_path)\n",
    "    df = pd.DataFrame(columns=trainval_sample.columns)\n",
    "    for i in trainval_sample.columns[14:]: df.loc[0, i]=trainval_sample.loc[0, i]\n",
    "    df.drop(['split'], axis=1, inplace=True)\n",
    "\n",
    "    def sums(length, total_sum):\n",
    "        if length == 1:\n",
    "            yield (total_sum,)\n",
    "        else:\n",
    "            for value in range(total_sum + 1):\n",
    "                for permutation in sums(length - 1, total_sum - value):\n",
    "                    yield (value,) + permutation\n",
    "    #Function to get difference between two lists\n",
    "    def mse(input, target): \n",
    "        difference = []\n",
    "        for inp, targ in zip(input, target): difference.append((inp-targ)**2)\n",
    "        return sum(difference)/len(difference)\n",
    "    #Function to count number of images in df\n",
    "    def count_img(lstlsts):\n",
    "        sum=0\n",
    "        for lst in lstlsts: sum+=len(lst)\n",
    "        return sum\n",
    "    #Function to get best combination for #time each image copied\n",
    "    def get_int_copy(sum, time_copy, error, new_range):\n",
    "        new_time_copy=[]\n",
    "        for com in sums(len(time_copy), sum):\n",
    "            if use_range:\n",
    "                if mse(time_copy, com)<error and max(time_copy)-min(time_copy)>new_range:\n",
    "                    new_time_copy=com\n",
    "                    error = mse(time_copy, new_time_copy)  \n",
    "                    new_range = max(time_copy)-min(time_copy)\n",
    "            else:\n",
    "                if mse(time_copy, com)<error:\n",
    "                    new_time_copy=com\n",
    "                    error = mse(time_copy, new_time_copy)\n",
    "        return new_time_copy\n",
    "    #Function to fill new_df\n",
    "    def filldf(new_time_copy, temp):\n",
    "        #Filling in new_df\n",
    "        temp_idx=0\n",
    "        for time in new_time_copy:\n",
    "            #find empty column and fill it with single df row's images column, repeat #time times\n",
    "            for column in range(time):\n",
    "                a = cp.copy(column)\n",
    "                while pd.notna(df.iloc[0, a]): a+=1\n",
    "                df.iloc[0, a] = temp[temp_idx]\n",
    "            temp_idx+=1\n",
    "            \n",
    "    #For combinations with one view\n",
    "    if len(input_views)<2:\n",
    "        time_copy = [] #Getting #time each row is multiplied combination\n",
    "        #If only one row present\n",
    "        if len(input_paths)<2: time_copy=[len(views)]\n",
    "        else:\n",
    "            #Minimize std of new combination\n",
    "            std = 7 #greater than max possible since std([1, 13])==6, and any longer list has smaller std\n",
    "            for com in sums(len(input_paths), len(views)):\n",
    "                if std>np.std(com):\n",
    "                    time_copy=com\n",
    "                    std=np.std(com)\n",
    "        #Sorting time_copy for indexing\n",
    "        time_copy.sort()\n",
    "        for time in range(1, len(time_copy)): time_copy[time] += time_copy[time-1]\n",
    "        time_copy.insert(0,0)\n",
    "        for time in range(1, len(time_copy)): df.loc[0, time_copy[time-1]:time_copy[time]] = input_paths[time-1]\n",
    "    else:  \n",
    "        #Checking if there are frontal/lateral views and main/other views\n",
    "        havelat = False\n",
    "        havefront = False\n",
    "        havemain = False\n",
    "        haveother = False\n",
    "        use_range=False\n",
    "        main_views = ['AP','PA','LATERAL','LL'] \n",
    "        for view in input_views: \n",
    "            temp = []\n",
    "            for i in input_paths: \n",
    "                check = str(i).split('/')[1].split('_')[:-1]\n",
    "                new_check = str()\n",
    "                if len(check)>1: new_check = \"_\".join(check)\n",
    "                else: new_check=check[0]\n",
    "                if new_check == view: temp.append(i)\n",
    "            df.loc[0, view] = temp[0] #Adding one example for each present column\n",
    "            if view in views[:7]:  havefront = True\n",
    "            else: havelat = True\n",
    "            if view in main_views: havemain = True\n",
    "            else: haveother = True  \n",
    "        havemainother = havemain and haveother\n",
    "        havefrontlat = havefront and havelat\n",
    "        #if only frontal/lateral views\n",
    "        if not havefrontlat:\n",
    "            #Get estimate for #times each image added to df_sample, but float estimates so next section fixes it\n",
    "            time_copy = []\n",
    "            #if only main/other views\n",
    "            if not havemainother:\n",
    "                for view in input_views:\n",
    "                    temp = []\n",
    "                    for i in input_paths: \n",
    "                        check = str(i).split('/')[1].split('_')[:-1]\n",
    "                        new_check = str()\n",
    "                        if len(check)>1: new_check = \"_\".join(check)\n",
    "                        else: new_check=check[0]\n",
    "                        if new_check == view: temp.append(i)\n",
    "                    for image in range(len(temp)):\n",
    "                        time_copy.append(len(views)/len(input_views)/len(temp))\n",
    "                        if not image: time_copy[-1]-=1\n",
    "            #if both main and other views\n",
    "            else:\n",
    "                use_range=True\n",
    "                for view in input_views:\n",
    "                    temp = []\n",
    "                    for i in input_paths: \n",
    "                        check = str(i).split('/')[1].split('_')[:-1]\n",
    "                        new_check = str()\n",
    "                        if len(check)>1: new_check = \"_\".join(check)\n",
    "                        else: new_check=check[0]\n",
    "                        if new_check == view: temp.append(i)\n",
    "                    for image in range(len(temp)):\n",
    "                        if view in main_views: \n",
    "                            if image: time_copy.append(1) #For images not in new_df\n",
    "                            else: time_copy.append(0) #For images already in new_df\n",
    "                        else: time_copy.append(len(views)/len(input_views)/len(temp))                                 \n",
    "            #Getting integer list of #times each image added to new_df, minimizing Euclidean distance\n",
    "            error=1000 #unsure of max MSE, so guessing\n",
    "            new_range=0\n",
    "            new_time_copy = get_int_copy(len(views)-len(input_views), time_copy, error, new_range)\n",
    "            filldf(new_time_copy, input_paths)\n",
    "        else: \n",
    "            #same as above except split between frontal/lateral categories instead of all len(views) columns\n",
    "            frontal = []\n",
    "            lateral = []\n",
    "            for view in input_views:\n",
    "                temp = []\n",
    "                for i in input_paths: \n",
    "                    check = str(i).split('/')[1].split('_')[:-1]\n",
    "                    new_check = str()\n",
    "                    if len(check)>1: new_check = \"_\".join(check)\n",
    "                    else: new_check=check[0]\n",
    "                    if new_check == view: temp.append(i)\n",
    "                if len(temp)>0:\n",
    "                    if view in views[:7]: frontal.append(temp)\n",
    "                    else: lateral.append(temp)\n",
    "            #Check if more images than #columns (len(views)/2) in frontal category\n",
    "            front_track = count_img(frontal)\n",
    "            if front_track>len(views)/2:\n",
    "                drop=1\n",
    "                lateral.append(frontal[-1][-drop:])\n",
    "                frontal[-1] = frontal[-1][:-drop] \n",
    "                front_track = count_img(frontal)\n",
    "                while front_track>len(views)/2:\n",
    "                    lateral[-1] = lateral[-1].append(frontal[-1][-drop:]) #pd.DataFrame.append\n",
    "                    frontal[-1] = frontal[-1][:-drop] \n",
    "                    front_track = count_img(frontal)\n",
    "                for view in views[7:]:\n",
    "                    if pd.notna(df.loc[0, view]): continue\n",
    "                    else: df.loc[0, view] = lateral[-1][0]\n",
    "                    break\n",
    "            frontal_time_copy = []\n",
    "            lateral_time_copy = []\n",
    "            for input_paths_list in frontal:       \n",
    "                for i in range(len(input_paths_list)):\n",
    "                    temp=0\n",
    "                    frontal_time_copy.append(len(views)/2/len(frontal)/len(input_paths_list))\n",
    "                    try: temp=frontal_time_copy[-1]\n",
    "                    except: frontal_time_copy.append(1) #For combinations with len=len(views)/2\n",
    "                    if not i: frontal_time_copy[-1]-=1\n",
    "            for input_paths_list in lateral:\n",
    "                check = str(input_paths_list[0]).split('/')[1].split('_')[:-1]\n",
    "                new_check = str()\n",
    "                if len(check)>1: new_check = \"_\".join(check)\n",
    "                else: new_check=check[0]\n",
    "                if new_check in views[:7]: #If frontal in lateral b/c len(frontal) was > len(views)/2\n",
    "                    for image in range(len(input_paths_list)): \n",
    "                        if image: lateral_time_copy.append(1) #For images not in new_df\n",
    "                        else: lateral_time_copy.append(0) #For images already in new_df\n",
    "                else:\n",
    "                    for i in range(len(input_paths_list)):\n",
    "                        lateral_time_copy.append(len(views)/2/len(lateral)/len(input_paths_list))\n",
    "                        if not i: lateral_time_copy[-1]-=1\n",
    "            #Getting integer list of #times each image added to df_sample, account for len(frontal/lateral)=1\n",
    "            error=1000 #unsure of max MSE, so guessing\n",
    "            if havemainother: use_range=True\n",
    "            new_range=0\n",
    "            notin=[1, 7]\n",
    "            if len(frontal_time_copy) in notin: new_frontal_time_copy = [int(item) for item in frontal_time_copy]\n",
    "            else: new_frontal_time_copy = get_int_copy(int(len(views)/2-len(frontal)), frontal_time_copy, error, new_range)\n",
    "            error = 1000\n",
    "            if len(lateral_time_copy) in notin: new_lateral_time_copy = [int(item) for item in lateral_time_copy]\n",
    "            else: new_lateral_time_copy = get_int_copy(int(len(views)/2-len(lateral)), lateral_time_copy, error, new_range)\n",
    "            #Combine list of dfs into single df\n",
    "            frontal = list(itertools.chain.from_iterable(frontal))\n",
    "            lateral = list(itertools.chain.from_iterable(lateral))\n",
    "            filldf(new_frontal_time_copy, frontal)\n",
    "            filldf(new_lateral_time_copy, lateral)\n",
    "    \n",
    "    single_repgen_trainval_sample_path = prep/'trainval_sample_repgen_single.csv'\n",
    "    vocab_path = Path('modules/repgen/vocab.pkl')\n",
    "    trainval_sample_single = pd.read_csv(single_repgen_trainval_sample_path)\n",
    "    trainval_sample_single['images']=prod_path/\"AP_0.dcm\"\n",
    "    train_sample_single = trainval_sample_single[trainval_sample_single['split']==False]\n",
    "    val_sample_single = trainval_sample_single[trainval_sample_single['split']==True]\n",
    "    train_sample_single.reset_index(drop=True, inplace=True)\n",
    "    val_sample_single.reset_index(drop=True, inplace=True)\n",
    "    with open(vocab_path, 'rb') as f: vocab = pickle.load(f)    \n",
    "        \n",
    "    isval=False\n",
    "    viewtype='images' \n",
    "    ispred=False\n",
    "    train_sample_dataset = RepGenDataset(train_sample_single,isval, viewtype, ispred, classes) \n",
    "    isval=True\n",
    "    val_sample_dataset = RepGenDataset(val_sample_single,isval, viewtype, ispred, classes) \n",
    "    bs=16\n",
    "    trainval_sample_dls = DataLoaders.from_dsets(train_sample_dataset, val_sample_dataset, bs=bs, device=cpu, create_batch=create_batch, num_workers=workers, shuffle=True)\n",
    "    trainval_sample_dls.valid = trainval_sample_dls.valid.new(shuffle=False)\n",
    "    \n",
    "    # Model settings (for visual extractor)\n",
    "    visual_extractor='resnet50' #'resnet101'\n",
    "    pretrained=True\n",
    "    # Model settings (for Transformer)  \n",
    "    num_layers=3 #number of layers of Transformer\n",
    "    d_model=512 #dimension of Transformer\n",
    "    d_ff=512 #dimension of FFN\n",
    "    num_heads=8 #number of heads in Transformer\n",
    "    dropout=0.1 #dropout rate of Transformer\n",
    "    use_bn = 0 #whether to use batch normalization\n",
    "    drop_prob_lm = 0.5\n",
    "    max_seq_len = 100\n",
    "    att_feat_size = 2048 #dimension of the patch features (d_vf in main.py)\n",
    "    ## Not used in original/current, but included in main.py\n",
    "    #parser.add_argument('--logit_layers', type=int, default=1, help='the number of the logit layer.') \n",
    "    # for Relational Memory    \n",
    "    rm_num_slots=3\n",
    "    rm_num_heads=8\n",
    "    rm_d_model=512\n",
    "    # for Sampling\n",
    "    beam_size = 3 #beam size when beam searching\n",
    "    group_size = 1\n",
    "    sample_n = 1 #sample number per image\n",
    "    sample_method = \"beam_search\" #sample methods to sample a report\n",
    "    temperature = 1.0 #temperature when sampling\n",
    "    output_logsoftmax = 1 #whether to output the probabilities\n",
    "    decoding_constraint = 0\n",
    "    block_trigrams = 1\n",
    "    # More params (not in main.py, but used in original/current)\n",
    "    diversity_lambda = 0.5       \n",
    "    input_encoding_size = 512\n",
    "    suppress_UNK = 0 \n",
    "    length_penalty = ''\n",
    "    mode='forward'\n",
    "    model = R2GenModel(visual_extractor,\n",
    "                    pretrained,\n",
    "                    num_layers,\n",
    "                    d_model,\n",
    "                    d_ff,\n",
    "                    num_heads,\n",
    "                    dropout,\n",
    "                    rm_num_slots,\n",
    "                    rm_num_heads,\n",
    "                    rm_d_model,\n",
    "                    vocab,\n",
    "                    input_encoding_size,\n",
    "                    drop_prob_lm,\n",
    "                    max_seq_len,\n",
    "                    att_feat_size,\n",
    "                    use_bn,\n",
    "                    beam_size,\n",
    "                    group_size,\n",
    "                    sample_n,\n",
    "                    sample_method,\n",
    "                    temperature,\n",
    "                    output_logsoftmax,\n",
    "                    decoding_constraint,\n",
    "                    block_trigrams,\n",
    "                    diversity_lambda,\n",
    "                    suppress_UNK,\n",
    "                    length_penalty,\n",
    "                    mode)\n",
    "    model = model.to(cpu)\n",
    "    \n",
    "    criterion = compute_loss\n",
    "    metrics = [bleu4] # bleu1, bleu2, bleu3, meteor, rouge, partial(precision, thresh=0.5), partial(recall, thresh=0.5), partial(f1, thresh=0.5)    \n",
    "    wd=5e-5\n",
    "    \n",
    "    learn = Learner(trainval_sample_dls, model, loss_func=criterion, wd=wd, \n",
    "                    splitter=rep_gen, metrics=metrics, cbs=SelectPred)\n",
    "    learn.load(\"repgen.0.0\", device=cpu)\n",
    "    learn.model.mode='sample'\n",
    "    def passfunc(arg): return arg #Make last arg for learn.predict to not decode anything\n",
    "    def decode(pred): #Convert idx_report to report\n",
    "        words = [] #For every word in report (size rep_len)\n",
    "        for report in pred:\n",
    "            for word in report: #For each word in report\n",
    "                txtword = vocab[word] #word = index for vocab\n",
    "                if txtword not in [word for word in vocab if word[:2]==\"xx\"]: words.append(txtword) \n",
    "        return \" \".join(words)\n",
    "    learn.dls.decode = passfunc\n",
    "    learn.dls.decode_batch = passfunc\n",
    "\n",
    "    reports = []\n",
    "    ispred=True\n",
    "    for i in input_views:\n",
    "        pred_dataset = RepGenDataset(df, isval, i, ispred, classes)\n",
    "        gts, rep, _ = learn.predict(pred_dataset[0])\n",
    "        if decode(rep)[-2:] != \" .\" or decode(rep)[-2:] != \". \": reports.append(decode(rep) + ' . ')\n",
    "        else: reports.append(decode(rep))\n",
    "    if len(input_views)<2:\n",
    "        report = reports[0]\n",
    "    else:\n",
    "        clean_first_view_report = [x.strip() for x in reports[0].split(\".\")]\n",
    "        more_views_report_append=[]\n",
    "        for i in range(1, len(input_views)):\n",
    "            more_views_report_append.append([x for x in reports[i].split(\".\") if x.strip() not in clean_first_view_report])\n",
    "        reports1 = []\n",
    "        reports1.extend(reports[0].split(\".\"))\n",
    "        for i in more_views_report_append: reports1.extend(i)\n",
    "        report = \".\".join(reports1)\n",
    "    df.loc[0, 'reports'] = report\n",
    "    \n",
    "    del trainval_sample_single \n",
    "    del train_sample_single\n",
    "    del val_sample_single\n",
    "    del train_sample_dataset\n",
    "    del val_sample_dataset\n",
    "    del trainval_sample_dls\n",
    "    del model\n",
    "    del learn\n",
    "    del pred_dataset\n",
    "    gc.collect()\n",
    "\n",
    "    month = str(datetime.now().month)\n",
    "    if int(month) < 10: month = \"0\"+str(month)\n",
    "    day = str(datetime.now().day)\n",
    "    if int(day) < 10: day = \"0\"+str(day)\n",
    "    df.loc[0, 'StudyElapsed'] = str(datetime.now().year)+'-'+month+'-'+day\n",
    "    make_date(df, 'StudyElapsed')\n",
    "    df['StudyElapsed'].values.astype(np.int64) // 10 ** 9\n",
    "    df.loc[0, 'Minutes'] = datetime.now().minute\n",
    "    df.loc[0, 'Hour'] = datetime.now().hour\n",
    "    df.loc[0, 'Seconds'] = datetime.now().second\n",
    "    df.loc[0, 'StudyWeek'] = datetime.now().isocalendar()[1]\n",
    "    df.loc[0, 'StudyDay'] = datetime.now().day\n",
    "    df.loc[0, 'StudyDayofweek'] = datetime.now().isocalendar()[2]\n",
    "    df.loc[0, 'StudyDayofyear'] = datetime.now().timetuple().tm_yday\n",
    "    df.loc[0, 'StudyElapsed'] = df['StudyElapsed'].values.astype(np.int64) // 10 ** 9\n",
    "    #\"\"\"\n",
    "\n",
    "    train_sample = trainval_sample[trainval_sample['split']==False]\n",
    "    val_sample = trainval_sample[trainval_sample['split']==True]\n",
    "    train_sample.reset_index(drop=True, inplace=True)\n",
    "    val_sample.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    size=224\n",
    "    seq_len=72\n",
    "    bs=1\n",
    "    temp = pd.concat([train_sample.iloc[:bs], val_sample.iloc[:bs]], ignore_index=True)\n",
    "    temp.loc[:, :14]=prod_path/\"AP_0.dcm\"\n",
    "    train_dls = []\n",
    "    val_dls = []\n",
    "    test_bs=1\n",
    "    test_workers = 0\n",
    "    pred_dls = []\n",
    "    \n",
    "    def vis_dls(bs, size, path, view, istest=None):\n",
    "        dblock = DataBlock(\n",
    "            blocks=(ImageBlock(cls=PILDicom2), MultiCategoryBlock(encoded=True, vocab=classes)),\n",
    "            get_x=ColReader(view),\n",
    "            get_y=ColReader(classes),\n",
    "            splitter=istest, \n",
    "            item_tfms=Resize(460),\n",
    "            batch_tfms=[IntToFloatTensor(div=2**16-1),\n",
    "                        Normalize.from_stats(*imagenet_stats),\n",
    "                        *aug_transforms(size=size)]) #, min_scale=0.75: RandomResizedCropGPU not working\n",
    "        if istest is None: return dblock.dataloaders(path, bs=bs, num_workers=test_workers) \n",
    "        else: return dblock.dataloaders(path, bs=bs, num_workers=workers) \n",
    "    for view in views: \n",
    "        train_dls.append(vis_dls(bs, size, temp, view, ColSplitter('split'))[0].to(\"cpu\"))\n",
    "        val_dls.append(vis_dls(bs, size, temp, view, ColSplitter('split'))[1].to(\"cpu\"))\n",
    "        pred_dls.append(vis_dls(test_bs, size, df, view)[0].to(\"cpu\"))\n",
    "        \n",
    "    with open(Path('./modules/txtcls/vocab.pkl'), 'rb') as f: vocab = pickle.load(f) \n",
    "    nomiss_repgen_test_path = prep/'test_repgen_nomiss.csv'\n",
    "    test = pd.read_csv(nomiss_repgen_test_path)\n",
    "    def txt_dls(bs, path, seq_len, istest=None):\n",
    "        dblock = DataBlock(\n",
    "            blocks=(TextBlock(tok_tfm=BaseTokenizer).from_df(text_cols='reports', vocab=vocab), \n",
    "                    MultiCategoryBlock(encoded=True, vocab=classes)),\n",
    "            get_x=ColReader(cols='text'),\n",
    "            get_y=ColReader(classes),\n",
    "            splitter=istest)                                         \n",
    "        if istest is None: return dblock.dataloaders(path, bs=bs, seq_len=seq_len, num_workers=test_workers) \n",
    "        else: return dblock.dataloaders(path, bs=bs, seq_len=seq_len, num_workers=workers)\n",
    "    train_dls.append(txt_dls(bs, temp, seq_len, ColSplitter('split'))[0].to(\"cpu\"))\n",
    "    val_dls.append(txt_dls(bs, temp, seq_len, ColSplitter('split'))[1].to(\"cpu\"))\n",
    "    pred_dls.append(txt_dls(test_bs, df.append(test.iloc[:1], ignore_index=True), seq_len)[0].to(\"cpu\"))\n",
    "    \n",
    "    cont_nn,cat_nn = cont_cat_split(temp, max_card=365, dep_var=classes)\n",
    "    for frame in [temp, df]:\n",
    "        frame[['Minutes', \n",
    "            'Hour', \n",
    "            'Seconds', \n",
    "            'StudyWeek', \n",
    "            'StudyDay', \n",
    "            'StudyDayofweek', \n",
    "            'StudyDayofyear',\n",
    "            'StudyElapsed']] = frame[['Minutes', \n",
    "                                     'Hour', \n",
    "                                     'Seconds', \n",
    "                                     'StudyWeek', \n",
    "                                     'StudyDay', \n",
    "                                     'StudyDayofweek', \n",
    "                                     'StudyDayofyear',\n",
    "                                     'StudyElapsed']].astype('int32')\n",
    "    def tab_dls(bs, path, is_test):\n",
    "        procs_nn = [Categorify, FillMissing, Normalize]\n",
    "        if is_test: \n",
    "            splits=None\n",
    "            works = test_workers\n",
    "        else:  \n",
    "            cond = (path.split==False)\n",
    "            train_idx = np.where( cond)[0]\n",
    "            valid_idx = np.where(~cond)[0]\n",
    "            splits = (list(train_idx),list(valid_idx))\n",
    "            works = workers\n",
    "        return TabularPandas(path, procs_nn, None, cont_nn, splits=splits, y_block=MultiCategoryBlock(encoded=True, vocab=classes), \n",
    "                              y_names=classes).dataloaders(bs, num_workers=works) #cat_nn[16:23] where None is\n",
    "    train_dls.append(tab_dls(bs, temp, False)[0].to(\"cpu\"))\n",
    "    val_dls.append(tab_dls(bs, temp, False)[1].to(\"cpu\"))\n",
    "    pred_dls.append(tab_dls(test_bs, df, True)[0].to(\"cpu\"))\n",
    "    \n",
    "    train_mixed_dl = SumDL(device, *train_dls)\n",
    "    valid_mixed_dl = SumDL(device, *val_dls)\n",
    "    mixed_dls = DataLoaders(train_mixed_dl, valid_mixed_dl)\n",
    "    pred_mixed_dls = SumDL(cpu, *pred_dls)\n",
    "    \n",
    "    a = pred_mixed_dls.one_batch()\n",
    "    \n",
    "    del train_dls\n",
    "    del val_dls\n",
    "    del pred_dls\n",
    "    gc.collect()\n",
    "    \n",
    "    drop_mult=0.5\n",
    "    model=xresnet18\n",
    "    txtcls_learn = text_classifier_learner(txt_dls(bs, temp, seq_len, ColSplitter('split')), AWD_LSTM, drop_mult=drop_mult)\n",
    "    unfreeze_name='lang.0.1'\n",
    "    txtcls_learn = txtcls_learn.load_encoder(unfreeze_name)\n",
    "\n",
    "    # Create our Multi-Modal model\n",
    "    sum_model = SumModel(cnn_learner(vis_dls(bs, size, temp, input_views[0], ColSplitter('split')), model).model,\n",
    "                         txtcls_learn.model, \n",
    "                         tabular_learner(tab_dls(bs, temp, False), layers=[500, 250]).model, \n",
    "                         len(classes))\n",
    "\n",
    "    # Set loss_scale for each loss\n",
    "    weights = [14/17/14, 14/17/14, 14/17/14, 14/17/14, 14/17/14, 14/17/14, 14/17/14, 14/17/14, \n",
    "               14/17/14, 14/17/14, 14/17/14, 14/17/14, 14/17/14, 14/17/14, 2/17, 0.25/17, 0.75/17]\n",
    "    thresh=0.5\n",
    "    loss_scale = 1.0\n",
    "    beta=1\n",
    "\n",
    "    loss = SumGradientBlending(1.0, *weights)\n",
    "    ap_w = partial(ap_weighted, weights=weights)\n",
    "\n",
    "    metrics = [ap_w]\n",
    "    \n",
    "    sum_learn = Learner(mixed_dls.to(\"cpu\"), sum_model.to(\"cpu\"), loss, splitter=sum_splitter, metrics=metrics)\n",
    "    name = 'sum.0.0'\n",
    "    sum_learn.load(name, device=cpu)\n",
    "    sum_learn.dls = sum_learn.dls.to(cpu)\n",
    "    sum_learn.model = sum_learn.model.to(cpu)\n",
    "    \n",
    "    del model\n",
    "    del sum_model\n",
    "    del txtcls_learn\n",
    "    del mixed_dls\n",
    "    gc.collect()\n",
    "    \n",
    "    preds,targs = sum_learn.get_preds(dl=[a]) #18 seconds\n",
    "    \n",
    "    def decode_prob(preds):\n",
    "        all_inp=0\n",
    "        preds = torch.stack(preds)\n",
    "        for weight in range(len(weights)): all_inp += preds[weight] * weights[weight]\n",
    "        preds = all_inp/len(weights)\n",
    "        preds = preds.sigmoid()\n",
    "        return preds\n",
    "    def decode_rep(preds, thresh=0.5):\n",
    "        all_inp=0\n",
    "        preds = torch.stack(preds)\n",
    "        for weight in range(len(weights)): all_inp += preds[weight] * weights[weight]\n",
    "        preds = all_inp/len(weights)\n",
    "        preds = preds.sigmoid()\n",
    "        preds[preds>=thresh] = 1\n",
    "        preds[preds<thresh] = 0\n",
    "        return preds\n",
    "    thresh = 0.5\n",
    "    confs = decode_prob(preds)\n",
    "    class_preds = decode_rep(preds, thresh)\n",
    "    confs_select = []\n",
    "    class_names = []\n",
    "    for i in range(len(class_preds[0])):\n",
    "        if class_preds[0][i]==1: \n",
    "            confs_select.append(confs[0][i].item())\n",
    "            class_names.append(classes[i])\n",
    "    confs_select_neg = []\n",
    "    class_names_neg = []\n",
    "    for i in range(len(class_preds[0])):\n",
    "        if class_preds[0][i]==0: \n",
    "            confs_select_neg.append(confs[0][i].item())\n",
    "            class_names_neg.append(classes[i])\n",
    "    class_names = [class_names for _, class_names in sorted(zip(confs_select, class_names))]\n",
    "    confs_select = sorted(confs_select, reverse=True)\n",
    "    class_names_neg = [class_names_neg for _, class_names_neg in sorted(zip(confs_select_neg, class_names_neg), reverse=True)]\n",
    "    confs_select_neg = sorted(confs_select_neg, reverse=True)\n",
    "    \n",
    "    summary.value += \"Given a confidence threshold of \"+str(thresh)+\",<br/> which is the minimum confidence the model must have in order to give a positive diagnosis for a disease,<br/> and is the ideal confidence for maximizing accuracy as determined by the validation set,<br/>\"\n",
    "    if len(class_names)<1:\n",
    "        summary.value += \"this patient's condition cannot be determined. Please contact them to collect another set of x-rays.<br/>\"\n",
    "    else:\n",
    "        summary.value += \"this patient most likely needs to get checked out for the following conditions:<br/>\"\n",
    "        for idx in range(len(class_names)-1):\n",
    "            summary.value += class_names[idx] + f\"({confs_select[idx]*100:.2f}% confident),<br/>\"\n",
    "        temp_idx = len(class_names)-1\n",
    "        summary.value += \"and \" + class_names[temp_idx] + f\"({confs_select[temp_idx]*100:.2f}% confident).<br/>\"\n",
    "\n",
    "        summary.value += \"<br/>This patient most likely doesn't need to get checked out for the following conditions:<br/>\"\n",
    "        for idx in range(len(class_names_neg)-1):\n",
    "            summary.value += class_names_neg[idx] + f\"({confs_select_neg[idx]*100:.2f}% confident),<br/>\"\n",
    "        temp_idx = len(class_names_neg)-1\n",
    "        summary.value += \"and \" + class_names_neg[temp_idx] + f\"({confs_select_neg[temp_idx]*100:.2f}% confident).<br/>\"\n",
    "    summary.value += ' </p>'\n",
    "    \n",
    "    def show_gradcam(learn, x, thresh):\n",
    "        class Hook():\n",
    "            def __init__(self, m): self.hook = m.register_forward_hook(self.hook_func)   \n",
    "            def hook_func(self, m, i, o): self.stored = o.detach().clone()\n",
    "            def __enter__(self, *args): return self\n",
    "            def __exit__(self, *args): self.hook.remove()\n",
    "        with torch.no_grad(): \n",
    "            output = learn.model.eval()(*x[:-1])\n",
    "            output = decode_rep(output, thresh)\n",
    "            class_idxes = [i for i, val in enumerate(output[0]) if val]\n",
    "        class HookBwd():\n",
    "            def __init__(self, m):\n",
    "                self.hook = m.register_backward_hook(self.hook_func)   \n",
    "            def hook_func(self, m, gi, go): self.stored = go[0].detach().clone()\n",
    "            def __enter__(self, *args): return self\n",
    "            def __exit__(self, *args): self.hook.remove()\n",
    "        for img in input_views:\n",
    "            def cmap(class_idx):\n",
    "                with HookBwd(learn.model.models[views.index(img)][0]) as hookg: \n",
    "                    with Hook(learn.model.models[views.index(img)][0]) as hook:\n",
    "                        output = learn.model.eval()(*x[:-1])\n",
    "                        act = hook.stored\n",
    "                    output[views.index(img)][0][class_idx].backward()\n",
    "                    grad = hookg.stored\n",
    "                return act, grad\n",
    "            for idx in class_idxes:\n",
    "                act, grad = cmap(idx)\n",
    "                w = grad[0].mean(dim=[1,2], keepdim=True)\n",
    "                cam_map = (w * act[0]).sum(0)\n",
    "                x_dec = TensorImage(PILDicom.create(df.loc[0, img]))\n",
    "                _,ax = plt.subplots()\n",
    "                x_dec.show(ctx=ax, cmap='gray')\n",
    "                ax.imshow(cam_map.detach().cpu(), alpha=0.6, extent=(0,x_dec.shape[0],x_dec.shape[1],0), interpolation='bilinear', cmap='magma');\n",
    "                plt.savefig(prod_path/Path(img+\",\"+classes[idx]+'.png'), bbox_inches='tight')\n",
    "    show_gradcam(sum_learn, a, thresh)\n",
    "       \n",
    "    def _eval_dropouts(mod):\n",
    "        module_name =  mod.__class__.__name__\n",
    "        if 'Dropout' in module_name or 'BatchNorm' in module_name: mod.training = False\n",
    "        for module in mod.children(): _eval_dropouts(module)\n",
    "    def intrinsic_attention(learn, batch, class_id=None):\n",
    "        \"Calculate the intrinsic attention of the input w.r.t to an output `class_id`, or the classification given by the model if `None`.\"\n",
    "        learn.model.models[len(views)].train()\n",
    "        _eval_dropouts(learn.model)\n",
    "        learn.model.models[len(views)].zero_grad()\n",
    "        learn.model.models[len(views)].reset()\n",
    "        batch = batch[len(views)]\n",
    "        emb = learn.model.models[len(views)][0].module.encoder(batch).detach().requires_grad_()\n",
    "        emb.retain_grad()\n",
    "        lstm = learn.model.models[len(views)][0].module(emb, True)\n",
    "        learn.model.models[len(views)].eval()\n",
    "        cl = learn.model.models[len(views)][1]((lstm, torch.zeros_like(batch).bool(),))[0].softmax(dim=-1)\n",
    "        if class_id is None: class_id = cl.argmax()\n",
    "        cl[0][class_id].backward()\n",
    "        attn = emb.grad.squeeze().abs().sum(dim=-1)\n",
    "        attn /= attn.max()\n",
    "        tok, _ = learn.dls.dls[len(views)].decode_batch((*tuplify(batch), *tuplify(cl)))[0]\n",
    "        \n",
    "        b = tok.split(\" . \")\n",
    "        for i in range(len(b)): b[i] = re.sub(r'(\\s)xx\\w+', \"\", b[i], flags=re.IGNORECASE)        \n",
    "        views1 = [view.lower() for view in views]\n",
    "        rep = dict(zip(views1, views))\n",
    "        def replace_all(text, dic):\n",
    "            for i, j in dic.items(): text = text.replace(\" \"+i, \" \"+j).replace(i+\" \", j+\" \").replace(\" \"+i+\" \", \" \"+j+\" \")\n",
    "            return text\n",
    "        c = [replace_all(x, rep) for x in b]\n",
    "        d = \". \".join(c)\n",
    "        e = d[:-1]\n",
    "        text = tool.correct(e)\n",
    "        text = re.sub(r'(\\s)xx\\w+', \"\", text, flags=re.IGNORECASE)   \n",
    "        if text[-1]!=\".\": text = text + \".\"\n",
    "        return text, attn\n",
    "    def value2rgba(x, cmap=cm.Purples, alpha_mult=1.0):\n",
    "        \"Convert a value `x` from 0 to 1 (inclusive) to an RGBA tuple according to `cmap` times transparency `alpha_mult`.\"\n",
    "        c = cmap(x)\n",
    "        rgb = (np.array(c[:-1]) * 255).astype(int)\n",
    "        a = c[-1] * alpha_mult\n",
    "        return tuple(rgb.tolist() + [a])\n",
    "    def piece_attn_html(pieces, attns, sep=' ', **kwargs):\n",
    "        html_code,spans = ['<span style=\"font-family: monospace;\">'], []\n",
    "        for p, a in zip(pieces, attns):\n",
    "            p = html.escape(p)\n",
    "            c = str(value2rgba(a, alpha_mult=0.5, **kwargs))\n",
    "            spans.append(f'<span title=\"{a:.3f}\" style=\"background-color: rgba{c};\">{p}</span>')\n",
    "        html_code.append(sep.join(spans))\n",
    "        html_code.append('</span>')\n",
    "        return ''.join(html_code)\n",
    "    def show_piece_attn(*args, **kwargs):\n",
    "        from IPython.display import display, HTML\n",
    "        display(HTML(piece_attn_html(*args, **kwargs)))\n",
    "    def html_intrinsic_attention(learn, x:tuple, class_id:int=None, **kwargs)->str:\n",
    "        text, attn = intrinsic_attention(learn, x, class_id)\n",
    "        return piece_attn_html(text.split(), to_np(attn), **kwargs)\n",
    "    def show_intrinsic_attention(learn, x:tuple, class_id:int=None, **kwargs)->None:\n",
    "        text, attn = intrinsic_attention(learn, x, class_id)\n",
    "        show_piece_attn(text.split(), to_np(attn), **kwargs)        \n",
    "    for i in range(len(class_preds[0])):\n",
    "        if class_preds[0][i].item()>0:\n",
    "            with open(prod_path/Path(classes[i]+'.txt'), \"wt\") as txt:\n",
    "                txt.write(html_intrinsic_attention(sum_learn, a, i))\n",
    "                txt.close()\n",
    "    #\"\"\"\n",
    "    def display_both(learn, x, thresh):\n",
    "        class Hook():\n",
    "            def __init__(self, m): self.hook = m.register_forward_hook(self.hook_func)   \n",
    "            def hook_func(self, m, i, o): self.stored = o.detach().clone()\n",
    "            def __enter__(self, *args): return self\n",
    "            def __exit__(self, *args): self.hook.remove()\n",
    "        with torch.no_grad(): \n",
    "            output = learn.model.eval()(*x[:-1])\n",
    "            output = decode_rep(output, thresh)\n",
    "            class_idxes = [i for i, val in enumerate(output[0]) if val]\n",
    "        class HookBwd():\n",
    "            def __init__(self, m):\n",
    "                self.hook = m.register_backward_hook(self.hook_func)   \n",
    "            def hook_func(self, m, gi, go): self.stored = go[0].detach().clone()\n",
    "            def __enter__(self, *args): return self\n",
    "            def __exit__(self, *args): self.hook.remove()\n",
    "\n",
    "        txts = []\n",
    "        for i in range(len(class_preds[0])):\n",
    "            if class_preds[0][i].item()>0:\n",
    "                with open(prod_path/Path(classes[i]+'.txt')) as txt:\n",
    "                    lines = txt.readlines()\n",
    "                    txts.append([lines[0]])\n",
    "        # Create a dataframe using pandas library\n",
    "        data = pd.DataFrame([[0, 0], [0, 0]], columns = ['Conditions', 'Report_Interpretation'])\n",
    "        for idx in class_idxes:\n",
    "            for i in input_views:\n",
    "                data.loc[class_idxes.index(idx), i] = str(prod_path)+\"/\"+str(i+\",\"+classes[idx]+'.png')\n",
    "            data.loc[class_idxes.index(idx), 'Conditions'] = classes[idx]\n",
    "            data.loc[class_idxes.index(idx), 'Report_Interpretation'] = txts[class_idxes.index(idx)] if type(txts[class_idxes.index(idx)])!=list else txts[class_idxes.index(idx)][0]\n",
    "        data.set_index('Conditions', inplace=True)\n",
    "\n",
    "        # Converting links to html tags\n",
    "        def path_to_image_html(path): \n",
    "            x_dec = TensorImage(PILDicom.create(df.loc[0, path.split(\"/\")[1].split(\",\")[0]]))\n",
    "            return '<img src=\"'+ path + '\" width=\"'+ str(int(x_dec.shape[0])) + '\" height=\"'+ str(int(x_dec.shape[1])) + '\">'\n",
    "\n",
    "        # Rendering the dataframe as HTML table\n",
    "        data.to_html(escape=False, formatters={col:path_to_image_html for col in input_views})\n",
    "        out_pl.clear_output()\n",
    "        with out_pl: display(HTML(data.to_html(escape=False,formatters={col:path_to_image_html for col in input_views}))) \n",
    "    display_both(sum_learn, a, thresh)\n",
    "    #\"\"\"            \n",
    "diagnose.on_click(on_click_classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VBox([heading, \n",
    "      ap_direct, \n",
    "      ap_btn_upload,\n",
    "      ap_axial_direct,\n",
    "      ap_axial_btn_upload,\n",
    "      ap_lld_direct,\n",
    "      ap_lld_btn_upload,\n",
    "      ap_rld_direct,\n",
    "      ap_rld_btn_upload,\n",
    "      pa_direct,\n",
    "      pa_btn_upload,\n",
    "      pa_lld_direct,\n",
    "      pa_lld_btn_upload,\n",
    "      pa_rld_direct,\n",
    "      pa_rld_btn_upload,\n",
    "      lat_direct, \n",
    "      lat_btn_upload, \n",
    "      ll_direct,\n",
    "      ll_btn_upload,\n",
    "      lao_direct,\n",
    "      lao_btn_upload,\n",
    "      rao_direct,\n",
    "      rao_btn_upload,\n",
    "      swim_direct,\n",
    "      swim_btn_upload,\n",
    "      xtab_lat_direct,\n",
    "      xtab_lat_btn_upload,\n",
    "      lpo_direct,\n",
    "      lpo_btn_upload,\n",
    "      diagnose,\n",
    "      summary,\n",
    "      out_pl],\n",
    "     layout=Layout(width='100%', display='flex', align_items='center'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
